{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision trees \n",
    "are very simple yet powerful supervised learning methods, which\n",
    "constructs a decision tree model, which will be used to make predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main advantage of this model is that a human\n",
    "being can easily understand and reproduce the sequence of decisions (especially if\n",
    "the number of attributes is small) taken to predict the target class of a new instance.\n",
    "This is very important for tasks such as medical diagnosis or credit approval, where\n",
    "we want to show a reason for the decision, rather than just saying this is what the\n",
    "training data suggests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem we would like to solve is to determine if a Titanic's passenger would\n",
    "have survived, given her age, passenger class, and sex."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "with open('titanic.csv') as csvfile:\n",
    "    titanic_reader = csv.reader(csvfile, delimiter=',', quotechar='\"')\n",
    "    \n",
    "    # Header contains feature names\n",
    "    row = next(titanic_reader)\n",
    "    feature_names = np.array(row)\n",
    "    \n",
    "    # Load dataset, and target classes\n",
    "    titanic_X, titanic_y = [], []\n",
    "    for row in titanic_reader:  \n",
    "        titanic_X.append(row)\n",
    "        titanic_y.append(row[2]) # The target value is \"survived\"\n",
    "    \n",
    "    titanic_X = np.array(titanic_X)\n",
    "    titanic_y = np.array(titanic_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['row.names' 'pclass' 'survived' 'name' 'age' 'embarked' 'home.dest' 'room'\n",
      " 'ticket' 'boat' 'sex'] ['1' '1st' '1' 'Allen, Miss Elisabeth Walton' '29.0000' 'Southampton'\n",
      " 'St Louis, MO' 'B-5' '24160 L221' '2' 'female'] 1\n"
     ]
    }
   ],
   "source": [
    "print (feature_names, titanic_X[0], titanic_y[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keep only class (1st,2nd,3rd), age (float), and sex (masc, fem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "titanic_X = titanic_X[:, [1, 4, 10]]\n",
    "feature_names = feature_names[[1, 4, 10]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have selected feature numbers 1, 4, and 10 that is class, age, and sex, based\n",
    "on the assumption that the remaining attributes have no effect on the passenger's\n",
    "survival. Feature selection is an extremely important step while creating a machine\n",
    "learning solution. If the algorithm does not have good features as input, it will not\n",
    "have good enough material to learn from, results won't be good, no matter even if\n",
    "we have the best machine learning algorithm ever designed.\n",
    "Sometimes the feature selection will be made manually, based on our knowledge\n",
    "of the problem's domain and the machine learning method we are planning to use.\n",
    "Sometimes feature selection may be done by using automatic tools to evaluate and\n",
    "select the most promising ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['pclass' 'age' 'sex']\n",
      "['1st' 'NA' 'female'] 1\n"
     ]
    }
   ],
   "source": [
    "print (feature_names)\n",
    "print (titanic_X[12],titanic_y[12])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have missing values, a usual problem with\n",
    "datasets. In this case, we decided to substitute missing values with the mean age in\n",
    "the training data. We could have taken a different approach, for example, using the\n",
    "most common value in the training data, or the median value. When we substitute\n",
    "missing values, we have to understand that we are modifying the original problem,\n",
    "so we have to be very careful with what we are doing. This is a general rule in\n",
    "machine learning; when we change data, we should have a clear idea of what we are\n",
    "changing, to avoid skewing the final results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We have missing values for age\n",
    "# Assign the mean value\n",
    "ages = titanic_X[:, 1]\n",
    "mean_age = np.mean(titanic_X[ages != 'NA', 1].astype(np.float))\n",
    "titanic_X[titanic_X[:, 1] == 'NA', 1] = mean_age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['pclass' 'age' 'sex']\n",
      "['1st' '31.19418104265403' 'female'] 1\n"
     ]
    }
   ],
   "source": [
    "print (feature_names)\n",
    "print (titanic_X[12],titanic_y[12])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our attributes (except for age) are categorical; that is,\n",
    "they correspond to a value taken from a discrete set such as male and female. So,\n",
    "we have to convert categorical data into real values. Let's start with the sex feature.\n",
    "The preprocessing module of scikit-learn includes a LabelEncoder class, whose fit\n",
    "method allows conversion of a categorical set into a 0..K-1 integer, where K is the\n",
    "number of different classes in the set (in the case of sex, just 0 or 1).This transformation implicitly introduces an\n",
    "ordering between classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical classes: ['female' 'male']\n",
      "Integer classes: [0 1]\n"
     ]
    }
   ],
   "source": [
    "# Encode sex \n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "enc = LabelEncoder()\n",
    "label_encoder = enc.fit(titanic_X[:, 2])\n",
    "print (\"Categorical classes:\", label_encoder.classes_)\n",
    "integer_classes = label_encoder.transform(label_encoder.classes_)\n",
    "print (\"Integer classes:\", integer_classes)\n",
    "t = label_encoder.transform(titanic_X[:, 2])\n",
    "titanic_X[:, 2] = t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last two sentences transform the values of the sex attribute into 0-1 values, and\n",
    "modify the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['pclass' 'age' 'sex']\n",
      "['1st' '31.19418104265403' '0'] 1\n"
     ]
    }
   ],
   "source": [
    "print (feature_names)\n",
    "print (titanic_X[12],titanic_y[12])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will try a more general approach that does not assume an ordering, and it\n",
    "is widely used to convert categorical classes into real-valued attributes. We will\n",
    "introduce an additional encoder and convert the class attributes into three new\n",
    "binary features, each of them indicating if the instance belongs to a feature value (1)\n",
    "or (0). This is called one hot encoding, and it is a very common way of managing\n",
    "categorical attributes for real-based methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have to convert the class. Since we have three different classes, we cannot convert to binary values (and using 0/1/2 values would imply an order, something we do not want). We use OneHotEncoder to get three different attributes.\n",
    "\n",
    "First converts the classes into integers and then uses the\n",
    "OneHotEncoder class to create the three new attributes that are added to the array of\n",
    "features. It finally eliminates from training data the original class feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical classes: ['1st' '2nd' '3rd']\n",
      "Integer classes: [[0]\n",
      " [1]\n",
      " [2]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "enc = LabelEncoder()\n",
    "label_encoder = enc.fit(titanic_X[:, 0])\n",
    "print (\"Categorical classes:\", label_encoder.classes_)\n",
    "integer_classes = label_encoder.transform(label_encoder.classes_).reshape(3, 1)\n",
    "print (\"Integer classes:\", integer_classes)\n",
    "enc = OneHotEncoder()\n",
    "one_hot_encoder = enc.fit(integer_classes)\n",
    "# First, convert clases to 0-(N-1) integers using label_encoder\n",
    "num_of_rows = titanic_X.shape[0]\n",
    "t = label_encoder.transform(titanic_X[:, 0]).reshape(num_of_rows, 1)\n",
    "# Second, create a sparse matrix with three columns, each one indicating if the instance belongs to the class\n",
    "new_features = one_hot_encoder.transform(t)\n",
    "# Add the new features to titanix_X\n",
    "titanic_X = np.concatenate([titanic_X, new_features.toarray()], axis = 1)\n",
    "#Eliminate converted columns\n",
    "titanic_X = np.delete(titanic_X, [0], 1)\n",
    "# Update feature names\n",
    "feature_names = ['age', 'sex', 'first_class', 'second_class', 'third_class']\n",
    "# Convert to numerical values\n",
    "titanic_X = titanic_X.astype(float)\n",
    "titanic_y = titanic_y.astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The preceding code first converts the classes into integers and then uses the\n",
    "OneHotEncoder class to create the three new attributes that are added to the array of\n",
    "features. It finally eliminates from training data the original class feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['age', 'sex', 'first_class', 'second_class', 'third_class']\n",
      "[ 31.19418104   0.           1.           0.           0.        ] 1.0\n"
     ]
    }
   ],
   "source": [
    "print (feature_names)\n",
    "print (titanic_X[12],titanic_y[12])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a decision tree classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Separate training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(titanic_X, titanic_y, test_size=0.25, random_state=33)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit a decision tree with the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "clf = tree.DecisionTreeClassifier(criterion='entropy', max_depth=3,min_samples_leaf=5)\n",
    "clf = clf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DecisionTreeClassifier accepts (as most learning methods) several\n",
    "hyperparameters that control its behavior. In this case, we used the Information\n",
    "Gain (IG) criterion for splitting learning data, told the method to build a tree of at\n",
    "most three levels, and to accept a node as a leaf if it includes at least five training\n",
    "instances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Entropy is a measure of disorder in a set, if we have zero entropy, it means all\n",
    "values are the same (in our case, all instances of the target classes are the same),\n",
    "while it reaches its maximum when there is an equal number of instances of each\n",
    "class (in our case, when half of the instances correspond to survivors and the other\n",
    "half to non survivors). At each node, we have a certain number of instances (starting\n",
    "from the whole dataset), and we measure its entropy. Our method will select the\n",
    "questions that yield more homogeneous partitions (with the lowest entropy), when\n",
    "we consider only those instances for which the answer for the question is yes or no,\n",
    "that is, when the entropy after answering the question decreases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Let's measure the accuracy of our method in the training set (we will first define a helper function to measure the performance of a classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:0.838 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "def measure_performance(X,y,clf, show_accuracy=True, show_classification_report=True, show_confusion_matrix=True):\n",
    "    y_pred=clf.predict(X)   \n",
    "    if show_accuracy:\n",
    "        print (\"Accuracy:{0:.3f}\".format(metrics.accuracy_score(y,y_pred)),\"\\n\")\n",
    "\n",
    "    if show_classification_report:\n",
    "        print (\"Classification report\")\n",
    "        print (metrics.classification_report(y,y_pred),\"\\n\")\n",
    "        \n",
    "    if show_confusion_matrix:\n",
    "        print (\"Confusion matrix\")\n",
    "        print (metrics.confusion_matrix(y,y_pred),\"\\n\")\n",
    "        \n",
    "measure_performance(X_train,y_train,clf, show_classification_report=False, show_confusion_matrix=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our tree has an accuracy of 0.838 on the training set. But remember that this is not\n",
    "a good indicator. This is especially true for decision trees as this method is highly\n",
    "susceptible to overfitting. Since we did not separate an evaluation set, we should\n",
    "apply cross-validation. \n",
    "\n",
    "For this example, we will use an extreme case of crossvalidation,\n",
    "named leave-one-out cross-validation. For each instance in the training\n",
    "sample, we train on the rest of the sample, and evaluate the model built on the only\n",
    "instance left out. After performing as many classifications as training instances,\n",
    "we calculate the accuracy simply as the proportion of times our method correctly\n",
    "predicted the class of the left-out instance, and found it is a little lower (as we\n",
    "expected) than the resubstitution accuracy on the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Perform leave-one-out cross validation to better measure performance, reducing variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import cross_val_score, LeaveOneOut\n",
    "from scipy.stats import sem\n",
    "\n",
    "def loo_cv(X_train,y_train,clf):\n",
    "    # Perform Leave-One-Out cross validation\n",
    "    # We are preforming 1313 classifications!\n",
    "    loo = LeaveOneOut(X_train[:].shape[0])\n",
    "    scores=np.zeros(X_train[:].shape[0])\n",
    "    for train_index,test_index in loo:\n",
    "        X_train_cv, X_test_cv= X_train[train_index], X_train[test_index]\n",
    "        y_train_cv, y_test_cv= y_train[train_index], y_train[test_index]\n",
    "        clf = clf.fit(X_train_cv,y_train_cv)\n",
    "        y_pred=clf.predict(X_test_cv)\n",
    "        scores[test_index]=metrics.accuracy_score(y_test_cv.astype(int), y_pred.astype(int))\n",
    "    print ((\"Mean score: {0:.3f} (+/-{1:.3f})\").format(np.mean(scores), sem(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean score: 0.837 (+/-0.012)\n"
     ]
    }
   ],
   "source": [
    "loo_cv(X_train, y_train,clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main advantage of leave-one-out cross-validation is that it allows almost as\n",
    "much data for training as we have available, so it is particularly well suited for those\n",
    "cases where data is scarce. Its main problem is that training a different classifier for\n",
    "each instance could be very costly in terms of the computation time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A common criticism to decision trees is that once the training set is divided after\n",
    "answering a question, it is not possible to reconsider this decision. For example, if\n",
    "we divide men and women, every subsequent question would be only about men or\n",
    "women, and the method could not consider another type of question (say, age less\n",
    "than a year, irrespective of the gender)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forests â€“ randomizing decisions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forests try to introduce some level\n",
    "of randomization in each step, proposing alternative trees and combining them to\n",
    "get the final prediction. These types of algorithms that consider several classifiers\n",
    "answering the same question are called ensemble methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forests propose to build a decision tree based on a subset of the training\n",
    "instances (selected randomly, with replacement), but using a small random number\n",
    "of features at each set from the feature set. This tree growing process is repeated\n",
    "several times, producing a set of classifiers. At prediction time, each grown tree,\n",
    "given an instance, predicts its target class exactly as decision trees do. The class\n",
    "that most of the trees vote (that is the class most predicted by the trees) is the one\n",
    "suggested by the ensemble classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean score: 0.817 (+/-0.012)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = RandomForestClassifier(n_estimators=10,random_state=33)\n",
    "clf = clf.fit(X_train,y_train)\n",
    "loo_cv(X_train,y_train,clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We find that results are actually worse for Random Forests. It seems that introducing\n",
    "randomization was, after all, not a good idea because the number of features was too\n",
    "small. However, for bigger datasets, with a bigger number of features, Random Forests\n",
    "is a very fast, simple, and popular method to improve accuracy, retaining the virtues of\n",
    "decision trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating the performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### let's measure the performance of decision trees on the testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:0.793 \n",
      "\n",
      "Classification report\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.77      0.96      0.85       202\n",
      "        1.0       0.88      0.54      0.67       127\n",
      "\n",
      "avg / total       0.81      0.79      0.78       329\n",
      " \n",
      "\n",
      "Confusion matrix\n",
      "[[193   9]\n",
      " [ 59  68]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf_dt=tree.DecisionTreeClassifier(criterion='entropy', max_depth=3,min_samples_leaf=5)\n",
    "clf_dt.fit(X_train,y_train)\n",
    "measure_performance(X_test,y_test,clf_dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the classification results and the confusion matrix, it seems that our method\n",
    "tends to predict too much that the person did not survive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
