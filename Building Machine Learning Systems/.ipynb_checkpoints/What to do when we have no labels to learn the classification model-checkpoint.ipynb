{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We will achieve this goal in this blog using clustering. This is a method of\n",
    "arranging items so that similar items are in one cluster and dissimilar items are in\n",
    "distinct ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Extract the salient features from each post and store it as a vector per post.\n",
    "2. Compute clustering on the vectors.\n",
    "3. Determine the cluster for the post in question.\n",
    "4. From this cluster, fetch a handful of posts that are different from the post in\n",
    "question. This will increase diversity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting raw text into a bag-of-words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(min_df=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameter min_df determines how CountVectorizer treats words that are not\n",
    "used frequently (minimum document frequency). If it is set to an integer, all words\n",
    "occurring less than that value will be dropped. If it is a fraction, all words that occur\n",
    "less than that fraction of the overall dataset will be dropped. The parameter max_df\n",
    "works in a similar manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that, as expected, the counting is done at word level (analyzer=word)\n",
    "and the words are determined by the regular expression pattern token_pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "content = [\"How to format my hard disk\", \" Hard disk format problems \"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = vectorizer.fit_transform(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['disk', 'format', 'hard', 'how', 'my', 'problems', 'to']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The vectorizer has detected seven words for which we can fetch the\n",
    "counts individually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [1 0]]\n"
     ]
    }
   ],
   "source": [
    "print(X.toarray().transpose())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that the first sentence contains all the words except for \"problems\", while\n",
    "the second contains all except \"how\", \"my\", and \"to\". \n",
    "\n",
    "From X, we can extract a feature vector\n",
    "that we can use to compare the two documents with each other.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will start with a naive approach to point out some preprocessing\n",
    "peculiarities we have to account for. \n",
    "\n",
    "So let us pick a random post, for which we\n",
    "will then create the count vector. \n",
    "\n",
    "We will then compare its distance to all the count\n",
    "vectors and fetch the post with the smallest one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counting words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### In this post dataset, we want to find the most similar post for the short post \"imaging databases\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#samples: 5, #features: 17\n",
      "['actual', 'capabl', 'contain', 'data', 'databas', 'imag', 'interest', 'learn', 'machin', 'perman', 'post', 'provid', 'safe', 'storag', 'store', 'stuff', 'toy']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import scipy as sp\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "DIR = r\"toy\"\n",
    "posts = [open(os.path.join(DIR, f)).read() for f in os.listdir(DIR)]\n",
    "\n",
    "new_post = \"imaging databases\"\n",
    "\n",
    "import nltk.stem\n",
    "english_stemmer = nltk.stem.SnowballStemmer('english')\n",
    "\n",
    "class StemmedCountVectorizer(CountVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super(StemmedCountVectorizer, self).build_analyzer()\n",
    "        return lambda doc: (english_stemmer.stem(w) for w in analyzer(doc))\n",
    "#This will perform the following steps for each post:\n",
    "#1. Lower casing the raw post in the preprocessing step (done in the parent class).\n",
    "#2. Extracting all individual words in the tokenization step (done in the parent\n",
    "#class).\n",
    "#3. Converting each word into its stemmed version.\n",
    "\n",
    "# vectorizer = CountVectorizer(min_df=1, stop_words='english',\n",
    "# preprocessor=stemmer)\n",
    "vectorizer = StemmedCountVectorizer(min_df=1, stop_words='english')\n",
    "\n",
    "\n",
    "X_train = vectorizer.fit_transform(posts)\n",
    "\n",
    "num_samples, num_features = X_train.shape\n",
    "print(\"#samples: %d, #features: %d\" % (num_samples, num_features))\n",
    "\n",
    "new_post_vec = vectorizer.transform([new_post])\n",
    "\n",
    "print(vectorizer.get_feature_names())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we can vectorize our new post as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 4)\t1\n",
      "  (0, 5)\t1 <class 'scipy.sparse.csr.csr_matrix'>\n"
     ]
    }
   ],
   "source": [
    "new_post_vec = vectorizer.transform([new_post])\n",
    "print(new_post_vec, type(new_post_vec))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the count vectors returned by the transform method are sparse. That is,\n",
    "each vector does not store one count value for each word, as most of those counts\n",
    "would be zero (post does not contain the word). Instead, it uses the more memory\n",
    "efficient implementation coo_matrix (for \"COOrdinate\")."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Via its member toarray(), we can again access full ndarray as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(new_post_vec.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to use the full array if we want to use it as a vector for similarity\n",
    "calculations. For the similarity measurement (the naive one), we calculate the\n",
    "Euclidean distance between the count vectors of the new post and all the old\n",
    "posts as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scipy as sp\n",
    "def dist_raw(v1, v2):\n",
    "    delta = v1-v2\n",
    "    return sp.linalg.norm(delta.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Post 0 with dist=1.41: This is a toy post about machine learning. Actually, it contains not much interesting stuff.\n",
      "=== Post 1 with dist=0.86: Imaging databases provide storage capabilities.\n",
      "=== Post 2 with dist=0.63: Most imaging databases safe images permanently.\n",
      "=== Post 3 with dist=0.77: Imaging databases store data.\n",
      "=== Post 4 with dist=0.77: Imaging databases store data. Imaging databases store data. Imaging databases store data.\n",
      "Best post is 2 with dist=0.63\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def dist_norm(v1, v2):\n",
    "    v1_normalized = v1 / sp.linalg.norm(v1.toarray())\n",
    "    v2_normalized = v2 / sp.linalg.norm(v2.toarray())\n",
    "\n",
    "    delta = v1_normalized - v2_normalized\n",
    "\n",
    "    return sp.linalg.norm(delta.toarray())\n",
    "\n",
    "dist = dist_norm\n",
    "best_dist = sys.maxsize\n",
    "best_i = None\n",
    "\n",
    "for i in range(0, num_samples):\n",
    "    post = posts[i]\n",
    "    if post == new_post:\n",
    "        continue\n",
    "    post_vec = X_train.getrow(i)\n",
    "    d = dist(post_vec, new_post_vec)\n",
    "\n",
    "    print(\"=== Post %i with dist=%.2f: %s\" % (i, d, post))\n",
    "\n",
    "    if d < best_dist:\n",
    "        best_dist = d\n",
    "        best_i = i\n",
    "\n",
    "print(\"Best post is %i with dist=%.2f\" % (best_i, best_dist))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at posts 3 and 4, however, the picture is not so clear any more. Post 4 is the\n",
    "same as Post 3, duplicated three times. So, it should also be of the same similarity to\n",
    "the new post as Post 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 1 1 1 0 0 0 0 0 0 0 0 1 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(X_train.getrow(3).toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 3 3 3 0 0 0 0 0 0 0 0 3 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(X_train.getrow(4).toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop words on steroids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The feature values simply count occurrences of terms in a post. counting term frequencies for every post, and in addition,\n",
    "discounting those that appear in many posts. In other words, we want a high value\n",
    "for a given term in a given value if that term occurs often in that particular post and\n",
    "very rarely anywhere else.\n",
    "\n",
    "This is exactly what term frequency â€“ inverse document frequency (TF-IDF)\n",
    "does; TF stands for the counting part, while IDF factors in the discounting. A naive\n",
    "implementation would look like the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def tfidf(t, d, D):\n",
    "    tf = float(d.count(t)) / sum(d.count(w) for w in set(d))\n",
    "    idf = sp.log(float(len(D)) / (len([doc for doc in D if t in doc])))\n",
    "    return tf * idf\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.270310072072\n",
      "0.0\n",
      "0.135155036036\n",
      "0.366204096223\n"
     ]
    }
   ],
   "source": [
    "a, abb, abc = [\"a\"], [\"a\", \"b\", \"b\"], [\"a\", \"b\", \"c\"]\n",
    "D = [a, abb, abc]\n",
    "\n",
    "print(tfidf(\"a\", a, D))\n",
    "print(tfidf(\"b\", abb, D))\n",
    "print(tfidf(\"a\", abc, D))\n",
    "print(tfidf(\"b\", abc, D))\n",
    "print(tfidf(\"c\", abc, D))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that a carries no meaning for any document since it is contained everywhere.\n",
    "b is more important for the document abb than for abc as it occurs there twice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StemmedTfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "            dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "            lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "            ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
      "            smooth_idf=True, stop_words='english', strip_accents=None,\n",
      "            sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "            tokenizer=None, use_idf=True, vocabulary=None)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "class StemmedTfidfVectorizer(TfidfVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super(StemmedTfidfVectorizer, self).build_analyzer()\n",
    "        return lambda doc: (english_stemmer.stem(w) for w in analyzer(doc))\n",
    "\n",
    "vectorizer = StemmedTfidfVectorizer(\n",
    "    min_df=1, stop_words='english')\n",
    "print(vectorizer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting document vectors will not contain counts any more. Instead, they will\n",
    "contain the individual TF-IDF values per term."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our current text preprocessing phase includes the following steps:\n",
    "1. Tokenizing the text.\n",
    "2. Throwing away words that occur way too often to be of any help in detecting\n",
    "relevant posts.\n",
    "3. Throwing away words that occur so seldom that there is only a small chance\n",
    "that they occur in future posts.\n",
    "4. Counting the remaining words.\n",
    "5. Calculating TF-IDF values from the counts, considering the whole\n",
    "text corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most clustering\n",
    "algorithms fall into one of the two methods, flat and hierarchical clustering.\n",
    "\n",
    "Flat clustering divides the posts into a set of clusters without relating the clusters to\n",
    "each other. The goal is simply to come up with a partitioning such that all posts in\n",
    "one cluster are most similar to each other while being dissimilar from the posts in all\n",
    "other clusters. Many flat clustering algorithms require the number of clusters to be\n",
    "specified up front.\n",
    "\n",
    "In hierarchical clustering, the number of clusters does not have to be specified.\n",
    "Instead, the hierarchical clustering creates a hierarchy of clusters. While similar posts\n",
    "are grouped into one cluster, similar clusters are again grouped into one uber-cluster.\n",
    "This is done recursively, until only one cluster is left, which contains everything. In\n",
    "this hierarchy, one can then choose the desired number of clusters. However, this\n",
    "comes at the cost of lower efficiency.\n",
    "\n",
    "http://scikit-learn.org/dev/modules/clustering.html\n",
    "    \n",
    "KMeans is a flat clustering method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialization complete\n",
      "Iteration  0, inertia 4.749\n",
      "Initialization complete\n",
      "Iteration  0, inertia 4.749\n",
      "Iteration  1, inertia 3.379\n",
      "Initialization complete\n",
      "Iteration  0, inertia 4.749\n",
      "Iteration  1, inertia 3.379\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from scipy.stats import norm\n",
    "from matplotlib import pylab\n",
    "seed = 2\n",
    "sp.random.seed(seed)  # to reproduce the data later on\n",
    "\n",
    "num_clusters = 3\n",
    "\n",
    "\n",
    "def plot_clustering(x, y, title, mx=None, ymax=None, xmin=None, km=None):\n",
    "    pylab.figure(num=None, figsize=(8, 6))\n",
    "    if km:\n",
    "        pylab.scatter(x, y, s=50, c=km.predict(list(zip(x, y))))\n",
    "    else:\n",
    "        pylab.scatter(x, y, s=50)\n",
    "\n",
    "    pylab.title(title)\n",
    "    pylab.xlabel(\"Occurrence word 1\")\n",
    "    pylab.ylabel(\"Occurrence word 2\")\n",
    "    # pylab.xticks([w*7*24 for w in range(10)], ['week %i'%w for w in range(10)])\n",
    "\n",
    "    pylab.autoscale(tight=True)\n",
    "    pylab.ylim(ymin=0, ymax=1)\n",
    "    pylab.xlim(xmin=0, xmax=1)\n",
    "    pylab.grid(True, linestyle='-', color='0.75')\n",
    "\n",
    "    return pylab\n",
    "\n",
    "xw1 = norm(loc=0.3, scale=.15).rvs(20)\n",
    "yw1 = norm(loc=0.3, scale=.15).rvs(20)\n",
    "\n",
    "xw2 = norm(loc=0.7, scale=.15).rvs(20)\n",
    "yw2 = norm(loc=0.7, scale=.15).rvs(20)\n",
    "\n",
    "xw3 = norm(loc=0.2, scale=.15).rvs(20)\n",
    "yw3 = norm(loc=0.8, scale=.15).rvs(20)\n",
    "\n",
    "x = sp.append(sp.append(xw1, xw2), xw3)\n",
    "y = sp.append(sp.append(yw1, yw2), yw3)\n",
    "\n",
    "i = 1\n",
    "plot_clustering(x, y, \"Vectors\")\n",
    "pylab.savefig(os.path.join(\"..\", \"1400_03_0%i.png\" % i))\n",
    "pylab.clf()\n",
    "\n",
    "i += 1\n",
    "\n",
    "#################### 1 iteration ####################\n",
    "\n",
    "mx, my = sp.meshgrid(sp.arange(0, 1, 0.001), sp.arange(0, 1, 0.001))\n",
    "\n",
    "km = KMeans(init='random', n_clusters=num_clusters, verbose=1,\n",
    "            n_init=1, max_iter=1,\n",
    "            random_state=seed)\n",
    "km.fit(sp.array(list(zip(x, y))))\n",
    "\n",
    "Z = km.predict(sp.c_[mx.ravel(), my.ravel()]).reshape(mx.shape)\n",
    "\n",
    "plot_clustering(x, y, \"Clustering iteration 1\", km=km)\n",
    "pylab.imshow(Z, interpolation='nearest',\n",
    "           extent=(mx.min(), mx.max(), my.min(), my.max()),\n",
    "           cmap=pylab.cm.Blues,\n",
    "           aspect='auto', origin='lower')\n",
    "c1a, c1b, c1c = km.cluster_centers_\n",
    "pylab.scatter(km.cluster_centers_[:, 0], km.cluster_centers_[:, 1],\n",
    "            marker='x', linewidth=2, s=100, color='black')\n",
    "pylab.savefig(os.path.join(\"..\", \"1400_03_0%i.png\" % i))\n",
    "pylab.clf()\n",
    "\n",
    "i += 1\n",
    "\n",
    "#################### 2 iterations ####################\n",
    "km = KMeans(init='random', n_clusters=num_clusters, verbose=1,\n",
    "            n_init=1, max_iter=2,\n",
    "            random_state=seed)\n",
    "km.fit(sp.array(list(zip(x, y))))\n",
    "\n",
    "Z = km.predict(sp.c_[mx.ravel(), my.ravel()]).reshape(mx.shape)\n",
    "\n",
    "plot_clustering(x, y, \"Clustering iteration 2\", km=km)\n",
    "pylab.imshow(Z, interpolation='nearest',\n",
    "           extent=(mx.min(), mx.max(), my.min(), my.max()),\n",
    "           cmap=pylab.cm.Blues,\n",
    "           aspect='auto', origin='lower')\n",
    "\n",
    "c2a, c2b, c2c = km.cluster_centers_\n",
    "pylab.scatter(km.cluster_centers_[:, 0], km.cluster_centers_[:, 1],\n",
    "            marker='x', linewidth=2, s=100, color='black')\n",
    "# import pdb;pdb.set_trace()\n",
    "pylab.gca().add_patch(\n",
    "    pylab.Arrow(c1a[0], c1a[1], c2a[0] - c1a[0], c2a[1] - c1a[1], width=0.1))\n",
    "pylab.gca().add_patch(\n",
    "    pylab.Arrow(c1b[0], c1b[1], c2b[0] - c1b[0], c2b[1] - c1b[1], width=0.1))\n",
    "pylab.gca().add_patch(\n",
    "    pylab.Arrow(c1c[0], c1c[1], c2c[0] - c1c[0], c2c[1] - c1c[1], width=0.1))\n",
    "\n",
    "pylab.savefig(os.path.join(\"..\", \"1400_03_0%i.png\" % i))\n",
    "pylab.clf()\n",
    "\n",
    "i += 1\n",
    "\n",
    "#################### 2 iterations ####################\n",
    "km = KMeans(init='random', n_clusters=num_clusters, verbose=1,\n",
    "            n_init=1, max_iter=2,\n",
    "            random_state=seed)\n",
    "km.fit(sp.array(list(zip(x, y))))\n",
    "\n",
    "Z = km.predict(sp.c_[mx.ravel(), my.ravel()]).reshape(mx.shape)\n",
    "\n",
    "plot_clustering(x, y, \"Clustering iteration 2\", km=km)\n",
    "pylab.imshow(Z, interpolation='nearest',\n",
    "           extent=(mx.min(), mx.max(), my.min(), my.max()),\n",
    "           cmap=pylab.cm.Blues,\n",
    "           aspect='auto', origin='lower')\n",
    "\n",
    "c2a, c2b, c2c = km.cluster_centers_\n",
    "pylab.scatter(km.cluster_centers_[:, 0], km.cluster_centers_[:, 1],\n",
    "            marker='x', linewidth=2, s=100, color='black')\n",
    "# import pdb;pdb.set_trace()\n",
    "pylab.gca().add_patch(\n",
    "    pylab.Arrow(c1a[0], c1a[1], c2a[0] - c1a[0], c2a[1] - c1a[1], width=0.1))\n",
    "pylab.gca().add_patch(\n",
    "    pylab.Arrow(c1b[0], c1b[1], c2b[0] - c1b[0], c2b[1] - c1b[1], width=0.1))\n",
    "pylab.gca().add_patch(\n",
    "    pylab.Arrow(c1c[0], c1c[1], c2c[0] - c1c[0], c2c[1] - c1c[1], width=0.1))\n",
    "\n",
    "pylab.savefig(os.path.join(\"..\", \"1400_03_0%i.png\" % i))\n",
    "pylab.clf()\n",
    "\n",
    "i += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'MLCOMP_DIR' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-48-71df4670d75a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcluster\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mKMeans\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m dataset = sklearn.datasets.load_mlcomp(\"20news-18828\", \"train\",\n\u001b[1;32m----> 4\u001b[1;33m                                        \u001b[0mmlcomp_root\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mMLCOMP_DIR\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m                                        categories=groups)\n\u001b[0;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Number of posts:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilenames\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'MLCOMP_DIR' is not defined"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "from sklearn.cluster import KMeans\n",
    "dataset = sklearn.datasets.load_mlcomp(\"20news-18828\", \"train\",\n",
    "                                       mlcomp_root=MLCOMP_DIR,\n",
    "                                       categories=groups)\n",
    "print(\"Number of posts:\", len(dataset.filenames))\n",
    "\n",
    "labels = dataset.target\n",
    "num_clusters = 50  # sp.unique(labels).shape[0]\n",
    "\n",
    "import nltk.stem\n",
    "english_stemmer = nltk.stem.SnowballStemmer('english')\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "class StemmedTfidfVectorizer(TfidfVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super(TfidfVectorizer, self).build_analyzer()\n",
    "        return lambda doc: (english_stemmer.stem(w) for w in analyzer(doc))\n",
    "\n",
    "vectorizer = StemmedTfidfVectorizer(min_df=10, max_df=0.5,\n",
    "                                    # max_features=1000,\n",
    "                                    stop_words='english', charset_error='ignore'\n",
    "                                    )\n",
    "vectorized = vectorizer.fit_transform(dataset.data)\n",
    "num_samples, num_features = vectorized.shape\n",
    "print(\"#samples: %d, #features: %d\" % (num_samples, num_features))\n",
    "\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "km = KMeans(n_clusters=num_clusters, init='k-means++', n_init=1,\n",
    "            verbose=1)\n",
    "\n",
    "clustered = km.fit(vectorized)\n",
    "\n",
    "from sklearn import metrics\n",
    "print(\"Homogeneity: %0.3f\" % metrics.homogeneity_score(labels, km.labels_))\n",
    "print(\"Completeness: %0.3f\" % metrics.completeness_score(labels, km.labels_))\n",
    "print(\"V-measure: %0.3f\" % metrics.v_measure_score(labels, km.labels_))\n",
    "print(\"Adjusted Rand Index: %0.3f\" %\n",
    "      metrics.adjusted_rand_score(labels, km.labels_))\n",
    "print(\"Adjusted Mutual Information: %0.3f\" %\n",
    "      metrics.adjusted_mutual_info_score(labels, km.labels_))\n",
    "print((\"Silhouette Coefficient: %0.3f\" %\n",
    "       metrics.silhouette_score(vectorized, labels, sample_size=1000)))\n",
    "\n",
    "new_post_vec = vectorizer.transform([new_post])\n",
    "new_post_label = km.predict(new_post_vec)[0]\n",
    "\n",
    "similar_indices = (km.labels_ == new_post_label).nonzero()[0]\n",
    "\n",
    "similar = []\n",
    "for i in similar_indices:\n",
    "    dist = sp.linalg.norm((new_post_vec - vectorized[i]).toarray())\n",
    "    similar.append((dist, dataset.data[i]))\n",
    "\n",
    "similar = sorted(similar)\n",
    "import pdb\n",
    "pdb.set_trace()\n",
    "show_at_1 = similar[0]\n",
    "show_at_2 = similar[len(similar) / 2]\n",
    "show_at_3 = similar[-1]\n",
    "\n",
    "print(show_at_1)\n",
    "print(show_at_2)\n",
    "print(show_at_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
